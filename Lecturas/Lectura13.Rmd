---
title: "Lectura13"
author: "Sofia Gacitua"
date: "2024-12-25"
output: html_document
---

## Regresión lineal mediante mínimos cuadrados

Condiciones para aplicar mínimos cuadrados:

1.  Las variables presentan una condición bivariante, por lo que, para cualquier valor fijo de X, los valores de Y se distribuyen normalmente con una varianza constante.
2.  La relación entre la variable X y las medias de la variable Y es lineal.
3.  Las observaciones de la muestra son independientes entre sí. Esto significa que no se puede usar regresión lineal con series de tiempo.

Script 13.1: Ajuste de una regresión lineal simple

```{r}
library(dplyr)
library(ggpubr)

# Cargar y filtrar los datos.
datos <- mtcars |> filter(wt > 2 & wt < 5)

# Ajustar modelo con R.
modelo <- lm(hp ~ disp, data = datos)

print(summary(modelo))

# Graficar los datos y el modelo obtenido
g1 <- ggscatter(datos, x = "disp", y = "hp",
                color = "steelblue", fill = "steelblue",
                ylab = "Potencia [hp]")

g1 <- g1 + geom_abline (intercept = coef(modelo)[1],
                        slope = coef (modelo) [2],
                        color = "red")

g1 <- g1 + xlab (bquote("Volumen útil de los cilindros" ~ group("[",
                                                                "in"~3, "]")))

# Definir valores del predictor para vehículos no incluidos
# en el conjunto mtcars
disp <- c(169.694, 230.214, 79.005, 94.085, 343.085,
          136.073, 357.305, 288.842, 223.128, 129.217,
          146.432, 193.474, 376.874, 202.566, 114.928)

# Usar el modelo para predecir el rendimiento de estosa modelos.
potencia_est <- predict(modelo, data.frame(disp))

# Graficar los valores predichos
nuevos <- data.frame(disp, hp = potencia_est)

g2 <- ggscatter(nuevos, x = "disp", y = "hp",
                color = "purple", fill = "purple",
                ylab = "Potencia [hp]")

g2 <- g2 + xlab(bquote ("Volumen útil de los cilindros" ~ group("[",
                                                                "in"~3, "]")))

# Unir los gráficos en uno solo
g1 <- ggpar(g1, xlim = c(75, 405), ylim = c(60, 340))
g2 <- ggpar(g2, xlim = c(75, 405), ylim = c(60, 340))
g <- ggarrange (g1, g2,
                labels = c("Modelo", "Predicciones"),
                hjust = c (-1.2, -0.7))
print(g)
```

## Regresión lineal con un predictor categórico

Script 13:2: Regresión lineal simple con un predictor dicotómico

```{r}
library(ggpubr)

# Obtener los datos.
datos <- mtcars |> filter (wt > 2 & wt < 5)

# Verificar correlación.
print(cor(datos[, c("hp", "am", "vs")]))

# Ajustar modelo con R.
modelo_vs <- lm(hp ~ vs, data = datos)
print(summary(modelo_vs))

# Graficar el modelo.
g1 <- ggscatter(datos, x = "vs", y = "hp",
                color = "steelblue", fill = "steelblue",
                xlab = "Forma del motor", ylab = "Potencia [hp]",
                xticks.by = 1)
g1 <- g1 + geom_abline(intercept = coef (modelo_vs) [1],
                        slope = coef(modelo_vs) [2],
                        color = "red")
print(g1)

# Graficar residuos.
residuos <- modelo_vs[["residuals"]]
datos <- cbind(datos, residuos)

g2 <- ggscatter(datos, x = "vs", y = "residuos",
                color = "steelblue", fill = "steelblue",
                xlab = "Forma del motor", ylab = "Residuos [hp]",
                xticks.by = 1)

g2 <- g2 + geom_hline(yintercept = 0, color = "red" )

# Unir los gráficos en uno solo
g <- ggarrange (g1, g2,
                labels = c("Modelo", "Residuos"),
                hjust = c (-2.5, -2.0))
print(g)
```

## Confiabilidad de un modelo de RLS

### Distribución e independencia

Script 13.3: Evaluación del modelo de regresión lineal simple usado como ejemplo

```{r}
library(car)
library(dplyr)
library(ggpubr)

# Cargar y filtrar los datos.
datos <- mtcars |> filter (wt > 2 & wt < 5)

# Ajustar modelo con R.
modelo <- lm(hp ~ disp, data = datos)

# Desplegar gráficos de residuos y mostrar pruebas de curvatura.
cat ("Pruebas de curvatura:\n")
residualPlots (modelo, type = "rstandard",
               id = list(method = "r", n = 3, cex = 0.7, location = "lr"),
               col = "steelblue", pch = 20, col.quad = "red")

# Verificar independencia de los residuos
set.seed (19)
db <- durbinWatsonTest(modelo)
cat ("\nPrueba de independencia:\n")
print(db)

# Desplegar gráficos marginales.
marginalModelPlots(modelo, sd = TRUE,
                   id = list (method = "r", n = 3, cex = 0.7,
                              location ="lr"),
                   col = "steelblue",
                   pch = 20,
                   col.line = c("steelblue", "red"))

# Prueba de la varianza del error no constante.
cat("\nPrueba de homocedasticidad:\n")
print(ncvTest(modelo))

# Desplegar gráficos de influencia.
casos_influyentes <- influencePlot(modelo, id = list (cex = 0.7))
cat("\nCasos que podrían ser influyentes: \n")
print(casos_influyentes)
```

## Calidad predictiva de un modelo de RLS

### Generalización

Script 13.4: Ajuste de una regresión lineal simple usando validación cruzada

```{r}
# Cargar y filtrar los datos.
datos <- mtcars |> filter (wt > 2 & wt < 5)
n <- nrow(datos)

# Crear conjuntos de entrenamiento y prueba.
set.seed(101)

n_entrenamiento <- floor(0.8 * n)
i_entrenamiento <- sample.int(n = n, size = n_entrenamiento,
                              replace = FALSE)

entrenamiento <- datos[i_entrenamiento, ]
prueba <- datos [-i_entrenamiento, ]

# Ajustar y mostrar el modelo con el conjunto de entrenamiento.
modelo <- lm(hp ~ disp, data = entrenamiento)
print(summary(modelo))

# Calcular error cuadrado promedio para el conjunto de entrenamiento.
rmse_entrenamiento <- sqrt(mean(resid(modelo) ** 2))
cat("MSE para el conjunto de entrenamiento:", rmse_entrenamiento, "\n")

# Hacer predicciones para el conjunto de prueba.
predicciones <- predict(modelo, prueba)

# Calcular error cuadrado promedio para el conjunto de prueba.
error <- prueba[["hp"]] - predicciones
rmse_prueba <- sqrt(mean(error ** 2))
cat("MSE para el conjunto de prueba:", rmse_prueba)
```

Script 13.5: Ajuste de una regresión lineal simple usando validación cruzada de cinco pliegues

```{r}
library(caret)
library(dplyr)

# Cargar y filtrar los datos.
datos <- mtcars |> filter(wt > 2 & wt < 5)
n <- nrow(datos)

# Ajustar y mostrar el modelo usando validación cruzada de 5 pliegues.
set.seed (111)
entrenamiento <- train(hp ~ disp, data = datos, method = "lm",
                       trControl = trainControl (method = "cv", number = 5))

modelo <- entrenamiento[["finalModel"]]
print(summary(modelo))

# Mostrar los resultados de cada pliegue.
cat("Errores en cada pliegue:\n")
print(entrenamiento[["resample"]])

# Mostrar el resultado estimado para el modelo.
cat("\nError estimado para el modelo:\n")
print(entrenamiento[["results"]])
```

Script 13.6: Ajuste de una regresión lineal simple usando validación cruzada dejando uno fuera

```{r}
library(caret)
library(dplyr)

# Cargar y filtrar los datos.
datos <- mtcars |> filter (wt > 2 & wt < 5)
n <- nrow (datos)

# Ajustar y mostrar el modelo usando validación cruzada de 5 pliegues.
set.seed(111)
entrenamiento <- train(hp ~ disp, data = datos, method = "lm",
                       trControl = trainControl (method = "LOOCV"))
modelo <- entrenamiento[["finalModel"]]
print(summary(modelo))

# Mostrar los errores.
cat("Predicciones en cada pliegue:\n")
print(entrenamiento[["pred"]])

# Mostrar el resultado estimado para el modelo.
cat("\nError estimado para el modelo:\n")
print(entrenamiento[["results"]])
```
